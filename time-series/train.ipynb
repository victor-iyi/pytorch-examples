{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a 2-layer LSTM network to find patterns in time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence(nn.Module):\n",
    "    def __init__(self, hidden_size=50):\n",
    "        super(Sequence, self).__init__()\n",
    "        \n",
    "        # Hidden layer.\n",
    "        self.hidden_size = hidden_size + 1\n",
    "        \n",
    "        # Network architecture.\n",
    "        self.lstm1 = nn.LSTMCell(input_size=1,\n",
    "                                 hidden_size=self.hidden_size)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=self.hidden_size,\n",
    "                                 hidden_size=self.hidden_size)\n",
    "        self.linear = nn.Linear(in_features=self.hidden_size,\n",
    "                                out_features=1)\n",
    "\n",
    "    def forward(self, inputs:torch.tensor, future:int=0):\n",
    "        input_size, n_inputs = inputs.size(0), inputs.size(1)\n",
    "        \n",
    "        # Hidden & cell state of 1st LSTM layer.\n",
    "        h_t1 = torch.zeros(input_size, self.hidden_size)\n",
    "        c_t1 = torch.zeros(input_size, self.hidden_size)\n",
    "\n",
    "        # Hidden & cell state for 2nd LSTM layer.\n",
    "        h_t2 = torch.zeros(input_size, self.hidden_size)\n",
    "        c_t2 = torch.zeros(input_size, self.hidden_size)\n",
    "        \n",
    "        # Outputs\n",
    "        outputs, output = [], 0\n",
    "\n",
    "        # Every input entry is a new time step.\n",
    "        for t, input_t in enumerate(inputs.chunk(n_inputs, dim=1)):\n",
    "            # h_t1, c_t1 = self.lstm1(input=input_t, weight=(h_t1, c_t1))\n",
    "            print(input_t.astype(torch.double))\n",
    "            h_t1, c_t1 = self.lstm1(input_t, (h_t1, c_t1))\n",
    "            h_t2, c_t2 = self.lstm2(h_t1, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        # If we should predict the future.\n",
    "        for t in range(future):\n",
    "            h_t1, c_t1 = self.lstm1(output, (h_t1, c_t1))\n",
    "            h_t2, c_t2 = self.lstm2(h_t1, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Create stacked torch tensor from outputs.\n",
    "        outputs = torch.stack(outputs, dim=1).squeeze(2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load the dataset.\n",
    "data_dir = '../datasets/time-series/sine-waves.pt'\n",
    "data = torch.load(data_dir)\n",
    "\n",
    "print('Data: {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing samples & skip frequency.\n",
    "n_test, skip = 3, 1\n",
    "\n",
    "# Training set.\n",
    "X_train = torch.from_numpy(data[n_test:, :-skip])\n",
    "y_train = torch.from_numpy(data[n_test:, skip:])\n",
    "\n",
    "# Testing set.\n",
    "X_test = torch.from_numpy(data[:n_test, :-skip])\n",
    "y_test = torch.from_numpy(data[:n_test, skip:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data shapes.\n",
    "print('X_train: {}\\ty_test: {}'.format(X_train.size(), y_train.size()))\n",
    "print('X_test: {}\\ty_test: {}'.format(X_test.size(), y_test.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "hidden_size = 50\n",
    "lr = 1e-1\n",
    "\n",
    "# Build model.\n",
    "model = Sequence(hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model structure:\\n{}'.format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function criterion.\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizer: LBFGS, since we can load the whole data to train.\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(inputs, target, t, future):\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    \n",
    "    \n",
    "    def draw(y_t, **kwargs):\n",
    "        size = inputs.size(0)\n",
    "        \n",
    "        plt.plot(np.arange(size), y_t[:size].numpy(), \n",
    "                 linestyle='solid', linewidth=2., **kwargs)\n",
    "        # plt.plot(np.arange(size, size + size), y_t[size:].numpy(), \n",
    "        #          linestyle='dashed', linewidth=2., **kwargs)\n",
    "    \n",
    "    # Plot values.\n",
    "    draw(target[0], color='r', label='1st index')\n",
    "    draw(target[1], color='g', label='2nd index')\n",
    "    draw(target[2], color='b', label='3rd index')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Predict future values for time sequences', fontsize=30)\n",
    "    \n",
    "    plt.xlabel('X-axis', fontsize=20)\n",
    "    plt.ylabel('Y-axis', fontsize=20)\n",
    "    \n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    \n",
    "    plt.savefig('predict-{:04d}.pdf'.format(t))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 15\n",
    "\n",
    "for t in range(steps):\n",
    "    print('\\nStep: {}'.format(t))\n",
    "    \n",
    "    def closure():\n",
    "        # Clear optimizer gradient buffer.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction.\n",
    "        pred = model(X_train)\n",
    "        \n",
    "        # Compute loss given prediction & grand truth.\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        print('Loss: {:.3f}'.format(loss.item()))\n",
    "        \n",
    "        # Compute gradient for loss w.r.t. trainable variables.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Return the loss (for optimizer to minimize).\n",
    "        return loss\n",
    "    \n",
    "    # Optimizer update step.\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    # Predict: No need to compute/track gradients\n",
    "    with torch.no_grad():\n",
    "        future = 1000\n",
    "        pred = model(X_test, future=future)\n",
    "        loss = criterion(pred[:, :-future], y_test)\n",
    "        print('Loss: {:.3f}'.format(loss.item()))\n",
    "        # `.detach()` to stop tensor from tracking history.\n",
    "        y = pred.detach().numpy()\n",
    "    \n",
    "    # Saves `.pdf` file on disk.\n",
    "    visualize(inputs=X_test, target=y_test, t=t, future=future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
